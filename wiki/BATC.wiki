#summary A Benchmark for Aggregate Techniques in Crowdsourcing.
#labels Phase-Deploy,Phase-Requirements,Featured,Phase-Support

= Introduction =

Today, crowdsourcing becomes a promising methdology to overcome various problems that require human knowledge such as image labelling, text annotation, and product recommendation. A wide range of applications (e.g. ESP game, reCaptcha, and Freebase) have been developed on top of more than 70 crowdsourcing platforms such as Amazon Mechanical Turk and CloudCrowd. The rapid growth of such crowdsourcing applications opens up a variety of interesting technical and social challenges.

One of the most critical issues of crowdsourcing is to aggregate different answers given by the crowd workers. This is a challenging task because of two reasons: (i) the workers might have wide ranging levels of expertise and (ii) the questions may vary in different levels of difficulty. While the former leads to the high contradiction and uncertainty in the answer set, the latter renders some difficulties in distinguishing between truthful workers and malicious workers.
To fully tackle this challenge, a rich body of research on answer aggregation has developed different techniques.

= Deployment & Launch Program =

This project is an Eclipse RCP applications based on Eclipse 4, therefore you will need Eclipse IDE and install Eclipse4 for deploying and lauching program. The detail steps are follwing:

  * Step 1: Check out CrowdBenchmark project from svn
  * Step 2: Follow section 3 and section 4 in this tutorial: http://www.vogella.com/articles/EclipseRCP/article.html#tutorial_installation
  * Step 3: Import CrowdBenchmark project to Eclipse RCP
  * Step 4: Launch project (see section 5 in Vogellat tutorial)  


= Download = 
  # Runnable Jar: click here
  # Example config files: config 1, config 2, config 3
  # Example data sets: //?
  # Demo video: [https://benchmarkcrowd.googlecode.com/files/CrowdBenchmark.mp4 download]

= User guide = 

System requirements: JDK 1.6. or above

  * Step 0: Deploy and launch program.
  * Step 1: Load a config or input parameter for Crowd, Question, Factor, Metric and Algorithm sections. 
  * Step 2: Run and evaluate the result of different algorithms.
  * Step 3: Change the config and re-run program. Arrange the views for comparison if necessary.

= Screenshot =